import os  # This helps our program talk to the computerâ€™s file system. Like finding your toy box, opening it, and putting toys (files) in or taking them out.
import json  # This helps us read and write files full of structured information (like a list of your favorite toys in a box called .json).
import glob  # This lets us look around and find lots of files that match a pattern. Like saying â€œfind all the red toy cars in the room!â€
import torch  # This is usually used to make AI models smart. We bring it to the party but we donâ€™t really use it in this code. Itâ€™s like an extra toy no one plays with.
import time  # This tells the program what time it is and helps it measure how long things take. Like setting a timer when we wait for cookies to bake. ğŸª
import aiohttp  # This helps us talk to websites (like sending a letter and getting a response, but with the internet).
import asyncio  # This lets our program do many things at once! Like listening to a story while playing with blocks.
from tqdm import tqdm  # This gives us a cool progress bar, but weâ€™re not playing with it right now.

# ğŸ¯ This is our helper function. It talks to the internet and waits for it to talk back.
async def fetch(session, url, payload, problem_id, save_path):
    try:
        # ğŸš€ We send our request to the internet with a package full of info (payload).
        async with session.post(url, json=payload, timeout=aiohttp.ClientTimeout(total=3000000)) as response:
            # ğŸ“¬ We wait for messages to come back slowly, one piece at a time.
            async for line in response.content:
                # ğŸ§© We turn the internet's words (text) into something our program can understand (a Python dictionary).
                data = json.loads(line.decode("utf-8"))
                
                # ğŸ’Œ We check: is there a message? Is there any useful content?
                if "message" in data and "content" in data["message"]:
                    content = data["message"]["content"]  # ğŸ“ Get the message text!
                    print(content, end="", flush=True)  # ğŸ“£ Show it on the screen immediately, so weâ€™re not waiting.

                    # ğŸ› ï¸ We clean the message to make it safe to write to a file.
                    formatted_paragraph = content.replace('\n', r'\n')  # Pretend new lines are plain text
                    formatted_paragraph = formatted_paragraph.replace("\\", r"\\")  # Make backslashes safe
                    formatted_paragraph = formatted_paragraph.replace('"', r'\"')  # Make double quotes safe
                    
                    # ğŸ“ Now we open a file with the name based on the problem ID and add our content there.
                    with open(os.path.join(save_path, f"{problem_id}_plans.txt"), 'a', encoding='utf-8') as f:
                        f.write(content + "\n")  # ğŸ–Šï¸ We write the message down like taking notes.

    except asyncio.TimeoutError:  # ğŸ•“ If we wait too long and get no answer...
        print("Request timed out")  # ğŸ“¢ We tell ourselves, â€œit took too long!â€

# ğŸ§  This part looks at one problem (like one puzzle), reads its files, and asks for help to solve it step by step.
async def process_problem(session, problem, args):
    start_time = time.time()  # â±ï¸ Start the timer to see how long it takes
    prob_path = problem  # ğŸ—‚ï¸ This is the folder with our problem inside
    print(f"\nProcessing problem: {prob_path}\n")  # ğŸ“£ Tell us what problem we're working on

    problem_id = int(os.path.basename(problem))  # ğŸ·ï¸ Grab the number at the end of the folder name, like Problem #42
    
    # ğŸ›‘ If we already did this problem before (file exists), we skip it!
    if os.path.exists(os.path.join(args.save_path, f"{problem_id}.txt")):
        return
    
    code_path = os.path.join(prob_path, "solutions.json")  # ğŸ“¦ This is the file where all our possible solutions are stored
    if not os.path.exists(code_path):  # ğŸ˜¢ If thereâ€™s no code there, we canâ€™t work with it
        return
    
    with open(code_path, 'r', encoding='utf-8') as f:
        code_list = json.load(f)  # ğŸ“– Open the box and see all the solutions inside
    min_code = min(code_list, key=lambda x: len(x))  # ğŸ§  Pick the shortest code (the easiest one to understand)

    question_file_path = os.path.join(prob_path, "question.txt")  # ğŸ“ƒ This file has the question or problem text
    with open(question_file_path, 'r', encoding='utf-8') as f:
        prompt_plan = f.read()  # ğŸ—£ï¸ We read the question like reading instructions

    # ğŸ§© Now we make the final input that weâ€™ll send to the AI model
    input_text = "code:\n" + min_code + "Write a step-by-step solution plan following the above code:\n"
    
    payload = {
        "model": "deepseek-r1:7b",  # ğŸ¤– This is the name of the smart assistant weâ€™re asking
        "reasoning_effort": 0,  # ğŸ›‹ï¸ This tells it not to think too hard (keep it simple)
        "narrative_style": "concise",  # ğŸ—¨ï¸ We want short and straight answers
        "messages": [  # ğŸ’Œ This is like a conversation
            {"role": "system", "content": "You are an assistant that helps programmers understand the code step by step in short responses.You always respond in english."},
            {"role": "user", "content": prompt_plan + input_text + "make your response short straight to the point and only use the given code do not generate code or think about another algorithm"}
        ],
        "temperature": 0.2,  # â„ï¸ Keep the answers focused and not too random
        "stream": False  # ğŸš« Donâ€™t send the answer in small pieces, give it all at once
    }

    # ğŸ“ Now we call our fetch function to talk to the model and get the plan
    await fetch(session, args.OLLAMA_API_URL, payload, problem_id, args.save_path)
    
    # â±ï¸ Check how much time we spent solving this one
    elapsed_time = time.time() - start_time
    print(f"\nTime taken to process problem {problem_id}: {elapsed_time:.2f} seconds")

# ğŸ§© This is where we go through many problems one by one
async def main(args):
    if os.name == 'nt':  # ğŸªŸ Special fix for Windows computers so they donâ€™t break when doing many things at once
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
        
    original_problems = glob.glob(os.path.join(args.test_path, '*'))  # ğŸ” Look for all problem folders
    problems = sorted(original_problems)  # ğŸ“š Put them in order
    
    # ğŸ˜µ If the user gave us a bad starting number, we stop!
    if args.start > len(problems) or args.start < 0:
        print(f"Start index {args.start} > number of problems {len(problems)}")
        return

    # ğŸ“ Decide which problems weâ€™ll do: from start to end
    start = args.start
    end = len(problems) if args.end is None or args.end > len(problems) else args.end
    problems = problems[start:end]  # ğŸ¯ Cut the list to just the ones we want
    
    async with aiohttp.ClientSession() as session:  # ğŸŒ Open our internet session
        for i, problem in enumerate(problems, 1):  # ğŸ” Go through each problem one by one
            print(f"Processing {i}/{len(problems)}...")  # ğŸ“£ Show our progress
            await process_problem(session, problem, args)  # ğŸ§  Work on the problem

# ğŸ This is where everything starts running
if __name__ == "__main__":
    import argparse  # ğŸ™ï¸ This helps us take commands from the user

    parser = argparse.ArgumentParser(description="Use Ollama to generate plan.")  # ğŸ“œ Set up how we talk to the script
    
    parser.add_argument("--test_path", default="", type=str, help='Path to test samples')  # ğŸ“ Where are the problems?
    parser.add_argument("--save_path", default="", type=str, help='Path to save plans')  # ğŸ“‚ Where do we save the answers?
    parser.add_argument("-s", "--start", default=0, type=int, help='Start index of test samples')  # ğŸ Where should we start?
    parser.add_argument("-e", "--end", default=5000, type=int, help='End index of test samples')  # ğŸ›‘ Where should we stop?
    parser.add_argument("-m", "--model", default="llama2", type=str, help='Ollama model name')  # ğŸ§  Which AI brain to use?
    parser.add_argument("-d", "--delay", default=1, type=int, help='Delay between requests in seconds')  # â²ï¸ Wait time between calls
    parser.add_argument("-u", "--OLLAMA_API_URL", default="http://localhost:11434/api/chat", type=str, help='Ollama API URL')  # ğŸŒ Where is our smart model?

    args = parser.parse_args()  # ğŸ› ï¸ Collect all the userâ€™s input settings
    asyncio.run(main(args))  # ğŸ§ƒ Letâ€™s start the main show!
